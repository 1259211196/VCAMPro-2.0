#import <Foundation/Foundation.h>
#import <UIKit/UIKit.h>
#import <AVFoundation/AVFoundation.h>
#import <VideoToolbox/VideoToolbox.h>
#import <CoreMedia/CoreMedia.h>
#import <CoreVideo/CoreVideo.h>
#import <objc/runtime.h>
#import <objc/message.h>

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wunguarded-availability-new"
#pragma clang diagnostic ignored "-Wdeprecated-declarations"

// ============================================================================
// ã€0. å·¥ä¸šçº§å®‰å…¨äº¤æ¢ç®—æ³•ã€‘
// ============================================================================
static void safe_swizzle(Class cls, SEL originalSelector, SEL swizzledSelector) {
    if (!cls) return;
    Method originalMethod = class_getInstanceMethod(cls, originalSelector);
    Method swizzledMethod = class_getInstanceMethod(cls, swizzledSelector);
    if (!originalMethod || !swizzledMethod) return;
    BOOL didAddMethod = class_addMethod(cls, originalSelector, method_getImplementation(swizzledMethod), method_getTypeEncoding(swizzledMethod));
    if (didAddMethod) { class_replaceMethod(cls, swizzledSelector, method_getImplementation(originalMethod), method_getTypeEncoding(originalMethod)); } 
    else { method_exchangeImplementations(originalMethod, swizzledMethod); }
}

// ============================================================================
// ã€1. æ— ç—•è½¬ç å¼•æ“ã€‘
// ============================================================================
@interface VCAMStealthPreprocessor : NSObject
+ (void)processVideoAtURL:(NSURL *)sourceURL completion:(void(^)(BOOL success))completion;
@end

@implementation VCAMStealthPreprocessor
+ (void)processVideoAtURL:(NSURL *)sourceURL completion:(void(^)(BOOL success))completion {
    NSString *destPath = [NSTemporaryDirectory() stringByAppendingPathComponent:@"com.apple.avfoundation.videocache.tmp"];
    [[NSFileManager defaultManager] removeItemAtPath:destPath error:nil];
    
    AVAsset *asset = [AVAsset assetWithURL:sourceURL];
    AVAssetExportSession *exportSession = [[AVAssetExportSession alloc] initWithAsset:asset presetName:AVAssetExportPresetHighestQuality];
    exportSession.outputURL = [NSURL fileURLWithPath:destPath];
    exportSession.outputFileType = AVFileTypeMPEG4;
    exportSession.shouldOptimizeForNetworkUse = YES;

    [exportSession exportAsynchronouslyWithCompletionHandler:^{
        dispatch_async(dispatch_get_main_queue(), ^{
            if (exportSession.status == AVAssetExportSessionStatusCompleted) {
                if (completion) completion(YES);
            } else {
                if (completion) completion(NO);
            }
        });
    }];
}
@end

// ============================================================================
// ã€2. å¯„ç”Ÿçº§æ¸²æŸ“å¼•æ“ (ğŸ‘‘ æ ¸å¿ƒï¼šå·å¤©æ¢æ—¥å…‹éš†å¼•æ“)ã€‘
// ============================================================================
@interface VCAMParasiteCore : NSObject
@property (nonatomic, strong) AVAssetReader *assetReader;
@property (nonatomic, strong) AVAssetReaderOutput *trackOutput;
@property (nonatomic, assign) VTPixelTransferSessionRef transferSession;
@property (nonatomic, strong) NSLock *readLock;
@property (nonatomic, assign) CVPixelBufferRef lastPixelBuffer;
@property (nonatomic, assign) BOOL isEnabled;
@property (nonatomic, assign) NSTimeInterval lastFrameTime;
@property (nonatomic, assign) NSTimeInterval videoFrameDuration;
+ (instancetype)sharedCore;
- (void)loadVideo;
- (CMSampleBufferRef)createInjectedSampleBufferFrom:(CMSampleBufferRef)originalBuffer;
@end

@implementation VCAMParasiteCore
+ (instancetype)sharedCore {
    static VCAMParasiteCore *core = nil;
    static dispatch_once_t onceToken;
    dispatch_once(&onceToken, ^{ core = [[VCAMParasiteCore alloc] init]; });
    return core;
}

- (instancetype)init {
    if (self = [super init]) {
        _readLock = [[NSLock alloc] init];
        _lastPixelBuffer = NULL;
        _isEnabled = YES; 
        _lastFrameTime = 0.0;
        _videoFrameDuration = 1.0 / 30.0;
        
        VTPixelTransferSessionCreate(kCFAllocatorDefault, &_transferSession);
        if (_transferSession) {
            VTSessionSetProperty(_transferSession, kVTPixelTransferPropertyKey_ScalingMode, kVTScalingMode_CropSourceToCleanAperture);
        }
        [self loadVideo];
    }
    return self;
}

- (void)loadVideo {
    NSString *videoPath = [NSTemporaryDirectory() stringByAppendingPathComponent:@"com.apple.avfoundation.videocache.tmp"];
    if (![[NSFileManager defaultManager] fileExistsAtPath:videoPath]) return;
    
    NSURL *videoURL = [NSURL fileURLWithPath:videoPath];
    AVAsset *asset = [AVAsset assetWithURL:videoURL];
    
    [asset loadValuesAsynchronouslyForKeys:@[@"tracks"] completionHandler:^{
        NSError *error = nil;
        if ([asset statusOfValueForKey:@"tracks" error:&error] != AVKeyValueStatusLoaded) return;
        
        dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_HIGH, 0), ^{
            [self.readLock lock];
            if (self.assetReader) {
                [self.assetReader cancelReading]; self.assetReader = nil; self.trackOutput = nil;
            }
            
            self.assetReader = [AVAssetReader assetReaderWithAsset:asset error:nil];
            AVAssetTrack *videoTrack = [[asset tracksWithMediaType:AVMediaTypeVideo] firstObject];
            
            if (videoTrack && self.assetReader) {
                float fps = videoTrack.nominalFrameRate;
                if (fps <= 0.0) fps = 30.0;
                self.videoFrameDuration = 1.0 / fps;
                
                NSDictionary *settings = @{
                    (id)kCVPixelBufferPixelFormatTypeKey: @(kCVPixelFormatType_32BGRA),
                    (id)kCVPixelBufferIOSurfacePropertiesKey: @{}
                };
                
                AVMutableVideoComposition *videoComp = nil;
                @try {
                    videoComp = (AVMutableVideoComposition *)[AVVideoComposition videoCompositionWithPropertiesOfAsset:asset];
                    if (!CGSizeEqualToSize(videoComp.renderSize, CGSizeZero)) {
                        videoComp.colorPrimaries = AVVideoColorPrimaries_ITU_R_709_2;
                        videoComp.colorTransferFunction = AVVideoTransferFunction_ITU_R_709_2;
                        videoComp.colorYCbCrMatrix = AVVideoYCbCrMatrix_ITU_R_709_2;
                    } else { videoComp = nil; }
                } @catch (NSException *e) { videoComp = nil; }
                
                if (videoComp) {
                    AVAssetReaderVideoCompositionOutput *compOut = [AVAssetReaderVideoCompositionOutput assetReaderVideoCompositionOutputWithVideoTracks:@[videoTrack] videoSettings:settings];
                    compOut.videoComposition = videoComp; self.trackOutput = (AVAssetReaderOutput *)compOut;
                } else {
                    self.trackOutput = [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:videoTrack outputSettings:settings];
                }
                
                if ([self.assetReader canAddOutput:self.trackOutput]) {
                    [self.assetReader addOutput:self.trackOutput]; [self.assetReader startReading];
                }
            }
            [self.readLock unlock];
        });
    }];
}

- (CVPixelBufferRef)copyNextFrame {
    if (!self.assetReader) return NULL;
    if (self.assetReader.status == AVAssetReaderStatusCompleted) { [self loadVideo]; }
    
    NSTimeInterval currentTime = [[NSDate date] timeIntervalSince1970];
    if (currentTime - self.lastFrameTime < self.videoFrameDuration) return NULL;
    
    if (self.assetReader.status == AVAssetReaderStatusReading) {
        CMSampleBufferRef sbuf = [self.trackOutput copyNextSampleBuffer];
        if (sbuf) {
            CVPixelBufferRef pix = CMSampleBufferGetImageBuffer(sbuf);
            if (pix) CVPixelBufferRetain(pix);
            CFRelease(sbuf);
            self.lastFrameTime = currentTime;
            return pix;
        } else { [self loadVideo]; }
    }
    return NULL;
}

// ğŸ‘‘ ç»ˆææ€æ‹›ï¼šç»ä¸è§¦ç¢°åŸç‰©ç†é”ï¼åŠ¨æ€å…‹éš†å¹¶è½¬ç§»ï¼
- (CMSampleBufferRef)createInjectedSampleBufferFrom:(CMSampleBufferRef)originalBuffer {
    CVPixelBufferRef origPix = CMSampleBufferGetImageBuffer(originalBuffer);
    if (!origPix) return NULL;

    [self.readLock lock];
    CVPixelBufferRef srcPix = [self copyNextFrame];
    [self.readLock unlock];

    if (srcPix) {
        if (_lastPixelBuffer) CVPixelBufferRelease(_lastPixelBuffer);
        _lastPixelBuffer = CVPixelBufferRetain(srcPix);
    } else {
        if (_lastPixelBuffer) srcPix = CVPixelBufferRetain(_lastPixelBuffer);
    }

    if (!srcPix) return NULL;

    // 1. æŠ“å– WhatsApp å½“å‰è¯·æ±‚çš„ç‰©ç†æ ¼å¼ï¼ˆåŠ¨æ€é€‚é…é€šè¯é™çº§ï¼‰
    OSType origFormat = CVPixelBufferGetPixelFormatType(origPix);
    size_t origW = CVPixelBufferGetWidth(origPix);
    size_t origH = CVPixelBufferGetHeight(origPix);

    // 2. å‡­ç©ºå…‹éš†ä¸€ä¸ªæ¯«æ— ç¡¬ä»¶é”çš„å…¨æ–°ç‰©ç†ç¼“å†²æ± ï¼Œå¹¶åŠ ä¸Š WebRTC ç¼–ç å¿…é¡»çš„ IOSurface å±æ€§
    CVPixelBufferRef newPix = NULL;
    NSDictionary *pixelAttributes = @{ (id)kCVPixelBufferIOSurfacePropertiesKey: @{} };
    CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, origW, origH, origFormat, (__bridge CFDictionaryRef)pixelAttributes, &newPix);
    
    if (status == kCVReturnSuccess && newPix) {
        // 3. å°†æˆ‘ä»¬çš„è§†é¢‘é€šè¿‡åº•å±‚ GPU æ— ç¼è½¬ç§»ï¼ˆè‡ªåŠ¨å˜é¢‘ã€æ¢è‰²ã€ç¼©æ”¾ï¼‰åˆ°æ–°ç¼“å†²æ± 
        if (self.transferSession) {
            VTPixelTransferSessionTransferImage(self.transferSession, srcPix, newPix);
        }

        // 4. å°†æ–°çš„ç¼“å†²æ± ä¼ªè£…æ‰“åŒ…æˆåˆæ³•çš„åŸç”Ÿ CMSampleBuffer
        CMSampleBufferRef newSampleBuffer = NULL;
        CMSampleTimingInfo timingInfo;
        if (CMSampleBufferGetSampleTimingInfo(originalBuffer, 0, &timingInfo) == kCMBlockBufferNoErr) {
            CMVideoFormatDescriptionRef videoInfo = NULL;
            CMVideoFormatDescriptionCreateForImageBuffer(kCFAllocatorDefault, newPix, &videoInfo);
            CMSampleBufferCreateForImageBuffer(kCFAllocatorDefault, newPix, true, NULL, NULL, videoInfo, &timingInfo, &newSampleBuffer);
            if (videoInfo) CFRelease(videoInfo);
        }
        CVPixelBufferRelease(newPix);
        return newSampleBuffer; // å®Œç¾å…‹éš†ä½“ç”Ÿæˆï¼
    }
    return NULL;
}
@end

// ============================================================================
// ã€3. éšå½¢ç¯å¢ƒä¼ªè£…ä»£ç† (ğŸ‘‘ ç»å¯¹é˜²å´©æºƒæ‹¦æˆªä½“ç³»)ã€‘
// ============================================================================
@interface VCAMStealthProxy : NSProxy <AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureAudioDataOutputSampleBufferDelegate>
@property (nonatomic, weak) id target;
@end

@implementation VCAMStealthProxy
+ (instancetype)proxyWithTarget:(id)target {
    VCAMStealthProxy *proxy = [VCAMStealthProxy alloc];
    proxy.target = target;
    return proxy;
}

// å¼ºåˆ¶é€šè¿‡ WhatsApp çš„ç±»éªŒè¯æ¢é’ˆ
- (NSMethodSignature *)methodSignatureForSelector:(SEL)sel { return [self.target methodSignatureForSelector:sel]; }
- (void)forwardInvocation:(NSInvocation *)invocation {
    if (self.target && [self.target respondsToSelector:invocation.selector]) { [invocation invokeWithTarget:self.target]; }
}
- (BOOL)respondsToSelector:(SEL)aSelector {
    if (aSelector == @selector(captureOutput:didOutputSampleBuffer:fromConnection:)) return YES;
    return [self.target respondsToSelector:aSelector];
}
- (BOOL)conformsToProtocol:(Protocol *)aProtocol { return [self.target conformsToProtocol:aProtocol]; }
- (BOOL)isKindOfClass:(Class)aClass { return [self.target isKindOfClass:aClass]; }

// æ‹¦æˆªæ¢çº½
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection {
    @autoreleasepool {
        if ([output isKindOfClass:NSClassFromString(@"AVCaptureVideoDataOutput")]) {
            if ([VCAMParasiteCore sharedCore].isEnabled) {
                // å°†å…‹éš†çš„å®Œç¾å‡ä½“äº¤ç»™ WhatsAppï¼Œä¸¢å¼ƒå¸¦æœ‰è¯»å†™é”çš„çœŸå®ç”»é¢ï¼
                CMSampleBufferRef fakeSample = [[VCAMParasiteCore sharedCore] createInjectedSampleBufferFrom:sampleBuffer];
                if (fakeSample) {
                    if ([self.target respondsToSelector:_cmd]) {
                        [(id)self.target captureOutput:output didOutputSampleBuffer:fakeSample fromConnection:connection];
                    }
                    CFRelease(fakeSample); // é‡Šæ”¾å†…å­˜ï¼Œé˜²å†…å­˜æ³„æ¼
                    return; // æ‹¦æˆªç»“æŸ
                }
            }
        } 
        else if ([output isKindOfClass:NSClassFromString(@"AVCaptureAudioDataOutput")]) {
            if ([VCAMParasiteCore sharedCore].isEnabled) {
                // ğŸ‘‘ éŸ³é¢‘æé™é˜²æŒ‚æ–­æ³•ï¼šå¹½çµä¸¢åŒ…æ³•
                // ç›´æ¥æ‹¦æˆªéŸ³é¢‘å¸§ï¼Œä¸å‘é€ç»™ WebRTCï¼è¿™ç­‰åŒäºç½‘ç»œéŸ³é¢‘ä¸¢åŒ…ï¼Œç»å¯¹ä¸ä¼šå¼•å‘å†…å­˜è¶Šç•ŒæŒ‚æ–­ï¼Œä¸”å®Œç¾å®ç°é™éŸ³ã€‚
                return;
            }
        }
        
        // æœªå¼€å¯æ’ä»¶æ—¶çš„å¸¸è§„æ”¾è¡Œ
        if ([self.target respondsToSelector:_cmd]) {
            [(id)self.target captureOutput:output didOutputSampleBuffer:sampleBuffer fromConnection:connection];
        }
    }
}
@end

// ============================================================================
// ã€4. éšèº«æ§åˆ¶å°ã€‘
// ============================================================================
@interface VCAMStealthUIManager : NSObject <UIImagePickerControllerDelegate, UINavigationControllerDelegate>
+ (instancetype)sharedManager;
- (void)showStealthMenuInWindow:(UIWindow *)window;
@end

@implementation VCAMStealthUIManager
+ (instancetype)sharedManager {
    static VCAMStealthUIManager *mgr = nil; static dispatch_once_t once;
    dispatch_once(&once, ^{ mgr = [[VCAMStealthUIManager alloc] init]; }); return mgr;
}

- (void)showStealthMenuInWindow:(UIWindow *)window {
    UIViewController *root = window.rootViewController;
    while (root.presentedViewController) root = root.presentedViewController;
    if (!root) return;
    
    UIAlertController *alert = [UIAlertController alertControllerWithTitle:@"ğŸ“¸ ç³»ç»Ÿè°ƒè¯•é€‰é¡¹" message:nil preferredStyle:UIAlertControllerStyleActionSheet];
    
    [alert addAction:[UIAlertAction actionWithTitle:[VCAMParasiteCore sharedCore].isEnabled ? @"ğŸŸ¢ è§†é¢‘æ³¨å…¥å·²å¼€å¯" : @"ğŸ”´ è§†é¢‘æ³¨å…¥å·²å…³é—­" style:UIAlertActionStyleDefault handler:^(UIAlertAction *a) {
        [VCAMParasiteCore sharedCore].isEnabled = ![VCAMParasiteCore sharedCore].isEnabled;
        UIImpactFeedbackGenerator *fb = [[UIImpactFeedbackGenerator alloc] initWithStyle:UIImpactFeedbackStyleHeavy]; [fb impactOccurred];
    }]];
    
    [alert addAction:[UIAlertAction actionWithTitle:@"ğŸ“ é€‰æ‹©æºè§†é¢‘æ–‡ä»¶" style:UIAlertActionStyleDefault handler:^(UIAlertAction *a) {
        UIImagePickerController *picker = [[UIImagePickerController alloc] init];
        picker.delegate = self;
        picker.sourceType = UIImagePickerControllerSourceTypePhotoLibrary;
        picker.mediaTypes = @[@"public.movie"];
        picker.videoExportPreset = AVAssetExportPresetPassthrough;
        [root presentViewController:picker animated:YES completion:nil];
    }]];
    
    [alert addAction:[UIAlertAction actionWithTitle:@"å–æ¶ˆ" style:UIAlertActionStyleCancel handler:nil]];
    
    if ([[UIDevice currentDevice] userInterfaceIdiom] == UIUserInterfaceIdiomPad) {
        alert.popoverPresentationController.sourceView = window;
        alert.popoverPresentationController.sourceRect = CGRectMake(window.bounds.size.width/2, window.bounds.size.height/2, 1, 1);
    }
    
    [root presentViewController:alert animated:YES completion:nil];
}

- (void)imagePickerController:(UIImagePickerController *)picker didFinishPickingMediaWithInfo:(NSDictionary *)info {
    NSURL *url = info[UIImagePickerControllerMediaURL];
    UIViewController *root = picker.presentingViewController;
    [picker dismissViewControllerAnimated:YES completion:^{
        if (url) {
            UIImpactFeedbackGenerator *fb = [[UIImpactFeedbackGenerator alloc] initWithStyle:UIImpactFeedbackStyleRigid]; [fb impactOccurred];
            [VCAMStealthPreprocessor processVideoAtURL:url completion:^(BOOL success) {
                if (success) {
                    [[VCAMParasiteCore sharedCore] loadVideo];
                    dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(0.3 * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{
                        UIImpactFeedbackGenerator *fb2 = [[UIImpactFeedbackGenerator alloc] initWithStyle:UIImpactFeedbackStyleHeavy]; [fb2 impactOccurred];
                    });
                } else {
                    UIAlertController *err = [UIAlertController alertControllerWithTitle:@"å¯¼å…¥å¤±è´¥" message:@"è§†é¢‘æ ¼å¼ä¸å…¼å®¹ã€‚" preferredStyle:UIAlertControllerStyleAlert];
                    [err addAction:[UIAlertAction actionWithTitle:@"ç¡®å®š" style:UIAlertActionStyleCancel handler:nil]];
                    [root presentViewController:err animated:YES completion:nil];
                }
            }];
        }
    }];
}
@end

// ============================================================================
// ã€5. æ ¸å¿ƒ Hook ä¸æ‰‹åŠ¿å¼ºåˆ¶ç©¿é€ã€‘
// ============================================================================
@interface VCAMGestureDelegate : NSObject <UIGestureRecognizerDelegate>
+ (instancetype)sharedDelegate;
@end
@implementation VCAMGestureDelegate
+ (instancetype)sharedDelegate { static VCAMGestureDelegate *instance = nil; static dispatch_once_t onceToken; dispatch_once(&onceToken, ^{ instance = [[VCAMGestureDelegate alloc] init]; }); return instance; }
- (BOOL)gestureRecognizer:(UIGestureRecognizer *)gestureRecognizer shouldRecognizeSimultaneouslyWithGestureRecognizer:(UIGestureRecognizer *)otherGestureRecognizer { return YES; }
@end

@implementation UIWindow (VCAMStealthHook)
- (void)vcam_setupGestures {
    if (!objc_getAssociatedObject(self, "_vcam_ges")) {
        UITapGestureRecognizer *tap = [[UITapGestureRecognizer alloc] initWithTarget:self action:@selector(vcam_handleTap:)];
        tap.numberOfTouchesRequired = 3; tap.numberOfTapsRequired = 1; 
        tap.cancelsTouchesInView = NO; tap.delaysTouchesBegan = NO;   
        tap.delegate = [VCAMGestureDelegate sharedDelegate]; 
        [self addGestureRecognizer:tap];
        objc_setAssociatedObject(self, "_vcam_ges", @YES, OBJC_ASSOCIATION_RETAIN_NONATOMIC);
    }
}
- (void)vcam_becomeKeyWindow { [self vcam_becomeKeyWindow]; [self vcam_setupGestures]; }
- (void)vcam_makeKeyAndVisible { [self vcam_makeKeyAndVisible]; [self vcam_setupGestures]; }
- (void)vcam_handleTap:(UITapGestureRecognizer *)g {
    if (g.state == UIGestureRecognizerStateRecognized) {
        [[VCAMStealthUIManager sharedManager] showStealthMenuInWindow:self];
        UIImpactFeedbackGenerator *fb = [[UIImpactFeedbackGenerator alloc] initWithStyle:UIImpactFeedbackStyleMedium]; [fb impactOccurred];
    }
}
@end

@implementation AVCaptureVideoDataOutput (VCAMStealthHook)
- (void)vcam_setSampleBufferDelegate:(id)delegate queue:(dispatch_queue_t)queue {
    if (delegate && ![delegate isKindOfClass:NSClassFromString(@"VCAMStealthProxy")]) {
        VCAMStealthProxy *proxy = [VCAMStealthProxy proxyWithTarget:delegate];
        objc_setAssociatedObject(self, "_vcam_video_p", proxy, OBJC_ASSOCIATION_RETAIN_NONATOMIC);
        [self vcam_setSampleBufferDelegate:proxy queue:queue];
    } else { [self vcam_setSampleBufferDelegate:delegate queue:queue]; }
}
@end

@implementation AVCaptureAudioDataOutput (VCAMStealthHook)
- (void)vcam_setSampleBufferDelegate:(id)delegate queue:(dispatch_queue_t)queue {
    if (delegate && ![delegate isKindOfClass:NSClassFromString(@"VCAMStealthProxy")]) {
        VCAMStealthProxy *proxy = [VCAMStealthProxy proxyWithTarget:delegate];
        objc_setAssociatedObject(self, "_vcam_audio_p", proxy, OBJC_ASSOCIATION_RETAIN_NONATOMIC);
        [self vcam_setSampleBufferDelegate:proxy queue:queue];
    } else { [self vcam_setSampleBufferDelegate:delegate queue:queue]; }
}
@end

// ============================================================================
// ã€6. å¯åŠ¨å™¨ã€‘
// ============================================================================
@interface VCAMLoader : NSObject
@end
@implementation VCAMLoader
+ (void)load {
    safe_swizzle([UIWindow class], @selector(becomeKeyWindow), @selector(vcam_becomeKeyWindow));
    safe_swizzle([UIWindow class], @selector(makeKeyAndVisible), @selector(vcam_makeKeyAndVisible));
    
    Class vdoClass = NSClassFromString(@"AVCaptureVideoDataOutput");
    if (vdoClass) safe_swizzle(vdoClass, @selector(setSampleBufferDelegate:queue:), @selector(vcam_setSampleBufferDelegate:queue:));
    
    Class adoClass = NSClassFromString(@"AVCaptureAudioDataOutput");
    if (adoClass) safe_swizzle(adoClass, @selector(setSampleBufferDelegate:queue:), @selector(vcam_setSampleBufferDelegate:queue:));
}
@end
#pragma clang diagnostic pop
